# AI Threats
A taxonomy of AI threat profiles, current and future. Contributions are welcomed! To contribute, please submit a PR.

## Capability Horizons
Threats are categorized into "horizons" based on the AI capability that we can reasonably expect will be required for the threat to initially emerge. Recognizing that any definition of capability will necessarily be imperfect, we nonetheless follow common conventions from AIS researchers and consider the following four categories of capability:

 **0. Current** - Threats that currently exist in the wild today. 

 **1. ANI** (*Artificial Narrow Intelligence*) - Threats that are already possible, or will soon become possible based on scaling and/or scaffolding the kinds of AI models that exist in the world today (e.g. GPT-4).

 **2. AGI** (*Artificial General Intelligence*) - Threats that become possble with the emergence of AGI. Here we make the following assumptions:

 * AGI has an agent model and the AI agents are capable of long-term planning.
 * AGI agents are at least as capable as an elite human (99th percentile) on any task of interest.
 * Agents can be horizontally scaled into the millions.

 **3. ASI** (*Artificial Superintelligence*) - 

## Maximum Impact 
Each threat profile has been assigned a "maximum impact". The idea here is to try to characterize and quantify the impact of the worst-case scenario if a threat of the given type emerges and is not contained.





